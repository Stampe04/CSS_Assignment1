{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e134fd45",
   "metadata": {},
   "source": [
    "Link to GitHub: https://github.com/Stampe04/CSS_Assignment1.git\n",
    "\n",
    "All group members contributed equally to this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d244ba",
   "metadata": {},
   "source": [
    "### Part 1: Ready Made vs Custom Made Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fe4db",
   "metadata": {},
   "source": [
    "***What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words).***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32001d",
   "metadata": {},
   "source": [
    "In Centola’s experiment, the custom-made data has the benefit of giving exactly what the study is looking for, without filling the data set with useless information or leaving out important information that would make the data set incomplete. However, the results might be impaired if any of the people participating were aware that they were a part of an experiment.\n",
    "\n",
    "In Nicolaides’s experiment, the ready-made data has the benefit of being always-on, meaning that they are constantly receiving new data that can be used in their experiment. However, as it was mentioned in the video, there might be other relevant insights about the people that they do not receive from their data or that they do not have access to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7f0b8",
   "metadata": {},
   "source": [
    "***How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc43dee",
   "metadata": {},
   "source": [
    "Centola's study can potentially help draw strong causal conclusions, if behaviour is significantly different between the 2 network structures, because the network structures are experimentally manipulated. However, this setup may not generalize well to real world social systems, where networks are influenced by many additional factors. \n",
    "\n",
    "Nicolaides' study can potentially create results that better reflect real world behaviour, since it uses naturally collected and large scale data. However, since the researchers cannot control for all variables, the results could only help draw correlational conclusions rather than causal. \n",
    "\n",
    "Comparing the two, Centola's results can better show how and why behaviour spreads, while Nicolaides' results can better show how patterns appear in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01c9c6",
   "metadata": {},
   "source": [
    "### Part 2: Find Researchers using the OpenAlex API ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f60cc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      7\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m data = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m/content/drive/MyDrive/Computational Social Science/ic2s2_2025_schedule_v5.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data = pd.read_csv('/content/drive/MyDrive/Computational Social Science/ic2s2_2025_schedule_v5.csv')\n",
    "\n",
    "# Find author column\n",
    "author_cols = [c for c in data.columns if \"author\" in c.lower()]\n",
    "print(\"Author-like columns:\", author_cols)\n",
    "\n",
    "author_col = author_cols[0] if author_cols else None\n",
    "assert author_col is not None, \"No author-like column found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_authors(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # normalize common separators\n",
    "    s = s.replace(\"\\n\", \";\")\n",
    "    s = re.sub(r\"\\s+(and|&)\\s+\", \";\", s, flags=re.IGNORECASE)\n",
    "    s = s.replace(\"|\", \";\")\n",
    "\n",
    "    parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "\n",
    "    # If still a single blob, try to detect comma-separated FULL NAMES:\n",
    "    # Example: \"Étienne Ollion, Émilien Schultz\"\n",
    "    if len(parts) == 1 and \", \" in parts[0]:\n",
    "        chunks = [c.strip() for c in parts[0].split(\",\") if c.strip()]\n",
    "\n",
    "        looks_like_full_names = sum((\" \" in c) for c in chunks) >= max(2, int(0.7 * len(chunks)))\n",
    "\n",
    "        if looks_like_full_names:\n",
    "            parts = chunks\n",
    "\n",
    "    return parts\n",
    "\n",
    "def clean_author_name(a):\n",
    "    a = re.sub(r\"\\s+\", \" \", str(a)).strip()\n",
    "    a = a.strip(\" '\\\"\")\n",
    "    a = a.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    return a.strip()\n",
    "\n",
    "all_authors = []\n",
    "for cell in data[author_col]:\n",
    "    all_authors.extend(split_authors(cell))\n",
    "\n",
    "authors_list = []\n",
    "seen = set()\n",
    "for a in all_authors:\n",
    "    a = clean_author_name(a)\n",
    "    if a and a not in seen:\n",
    "        seen.add(a)\n",
    "        authors_list.append(a)\n",
    "\n",
    "print(\"Number of unique authors:\", len(authors_list))\n",
    "print(authors_list[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#api_key = \"MQ3Jfhzi1EOFIYdbkDlnRf\"\n",
    "#api_key = \"qLZSYk0X9NhCmgsdvcGRNJ\"\n",
    "#api_key = \"JW3vNYrvustrbEkExjEvWH\"\n",
    "api_key = \"c3nPPdLKjAfWMJKxPtzPR6\"\n",
    "BASE = \"https://api.openalex.org\"\n",
    "\n",
    "# OpenAlex helper for data retrieval\n",
    "def oa_get(path, params=None, sleep=0.6, tries=3):\n",
    "    url = f\"{BASE}{path}\"\n",
    "    params = dict(params or {})\n",
    "    params[\"api_key\"] = api_key  # or remove this line and use \"mailto\" instead if you don't have an API key\n",
    "\n",
    "    last_err = None\n",
    "\n",
    "    for _ in range(tries):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=60)\n",
    "            time.sleep(sleep)\n",
    "            #print(\"Status:\", r.status_code, \"URL:\", r.url)\n",
    "\n",
    "            # if non-200, print body and retry\n",
    "            if r.status_code != 200:\n",
    "                print(\"Non-200 response:\", r.text[:500])\n",
    "                last_err = f\"HTTP {r.status_code}\"\n",
    "                time.sleep(1.0)\n",
    "                continue\n",
    "\n",
    "            js = r.json()\n",
    "\n",
    "            if isinstance(js, dict) and \"error\" in js:\n",
    "                print(\"OpenAlex error JSON:\", js)\n",
    "                last_err = js\n",
    "                time.sleep(1.0)\n",
    "                continue\n",
    "\n",
    "            return js\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception in oa_get:\", e)\n",
    "            last_err = e\n",
    "            time.sleep(1.0)\n",
    "\n",
    "    print(\"oa_get failed, last_err:\", last_err)\n",
    "    return None\n",
    "\n",
    "# OpenAlex helper for reading through multiple pages\n",
    "def paginate(path, params=None, per_page=200, max_pages=None):\n",
    "    \"\"\"Generator over all results using cursor pagination.\"\"\"\n",
    "    p = dict(params or {})\n",
    "    p[\"per-page\"] = per_page\n",
    "    p[\"cursor\"] = \"*\"\n",
    "\n",
    "    page = 0\n",
    "    while True:\n",
    "        js = oa_get(path, p)\n",
    "        results = js.get(\"results\", [])\n",
    "        for item in results:\n",
    "            yield item\n",
    "\n",
    "        next_cursor = js.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        page += 1\n",
    "\n",
    "        if not next_cursor:\n",
    "            break\n",
    "        if max_pages is not None and page >= max_pages:\n",
    "            break\n",
    "\n",
    "        p[\"cursor\"] = next_cursor\n",
    "\n",
    "# Takes the concept name (like Sociology and Computer Science) and returns a concept ID\n",
    "def get_concept_id_by_name(name):\n",
    "    js = oa_get(\"/concepts\", {\n",
    "        \"search\": name,\n",
    "        \"select\": \"id,display_name,level,works_count\",\n",
    "        \"per-page\": 10\n",
    "    })\n",
    "    results = js.get(\"results\", [])\n",
    "    if not results:\n",
    "        return None\n",
    "\n",
    "    name_l = name.strip().lower()\n",
    "    for c in results:\n",
    "        if (c.get(\"display_name\") or \"\").strip().lower() == name_l:\n",
    "            return c.get(\"id\")\n",
    "\n",
    "    return results[0].get(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61032a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_author_tophit(author_name):\n",
    "    js = oa_get(\"/authors\", {\n",
    "        \"search\": author_name,\n",
    "        \"select\": \"id,display_name,works_count,summary_stats,last_known_institutions\",\n",
    "        \"per-page\": 1\n",
    "    })\n",
    "    if js is None:\n",
    "        return None\n",
    "\n",
    "    results = js.get(\"results\", [])\n",
    "    if not results:\n",
    "        return None\n",
    "\n",
    "    r = results[0]\n",
    "    insts = r.get(\"last_known_institutions\") or []\n",
    "    country_code = insts[0].get(\"country_code\") if insts else None\n",
    "\n",
    "    return {\n",
    "        \"id\": r.get(\"id\"),\n",
    "        \"display_name\": r.get(\"display_name\"),\n",
    "        \"works_count\": r.get(\"works_count\"),\n",
    "        \"h_index\": (r.get(\"summary_stats\") or {}).get(\"h_index\"),\n",
    "        \"country_code\": country_code,\n",
    "        \"query_name\": author_name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = authors_list[0]\n",
    "print(\"Testing search for:\", name)\n",
    "\n",
    "js = oa_get(\"/authors\", {\n",
    "    \"search\": name,\n",
    "    \"select\": \"id,display_name,works_count,summary_stats,last_known_institutions\",\n",
    "    \"per-page\": 1\n",
    "})\n",
    "print(\"Raw js:\", js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = authors_list[0]\n",
    "print(\"Testing search for:\", test_name)\n",
    "\n",
    "test = oa_get(\"/authors\", {\n",
    "    \"search\": test_name,\n",
    "    \"select\": \"id,display_name,works_count\",\n",
    "    \"per-page\": 1\n",
    "})\n",
    "print(test[\"results\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37571158",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = []\n",
    "\n",
    "for nm in authors_list[:10]:\n",
    "    r = find_author_tophit(nm)\n",
    "    print(nm, \"->\", \"OK\" if r else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287dc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_authors = 100  # write None to go through all authors\n",
    "\n",
    "if max_authors is None:\n",
    "    to_iterate = authors_list\n",
    "else:\n",
    "    to_iterate = authors_list[:max_authors]\n",
    "\n",
    "D1 = []\n",
    "\n",
    "for nm in to_iterate:\n",
    "    res = find_author_tophit(nm)\n",
    "    if res is not None:\n",
    "        D1.append(res)\n",
    "\n",
    "df_auth = pd.DataFrame(D1)\n",
    "print(\"Matched OpenAlex authors:\", len(df_auth))\n",
    "display(df_auth.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1716b8",
   "metadata": {},
   "source": [
    "***Which challenges did you encounter? How did you address them?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f673bfa",
   "metadata": {},
   "source": [
    "In the beginning, we had some issues with the API itself, since we found the API documentation very confusing to navigate on their homepage. However, this slowly became less of an issue as we went along and got more familiar with the documentation and the code implementation. Furthermore, We had some initial issues loading the .csv file properly, but this was solved by doing some simple research on handling .csv files in Python. In addition, we ran into trouble with our API keys multiple times, as we kept running into limitation issues, e.g. not having enough tokens per day. We solved this by testing our code on a limited amount of data to use our keys sparingly, and then do the full run once we knew the code worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac330559",
   "metadata": {},
   "source": [
    "***Choose one problem you faced while collecting the data and describe your solution. Why did you choose this approach, and what impact might it have on your data?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1a98b",
   "metadata": {},
   "source": [
    "The biggest problem would be the limitation issues with our API keys, as we tested whole tables during our coding trials which put quite a strain on our amount of token usage. We solved this problem by limiting the number of calls with our API keys, e.g. limiting the number of data we called to ensure the code worked for all rows of data. We chose this approach as we still needed to test our code for edge cases and this would do just that without limiting our testing to a few times a day.\n",
    "\n",
    "Another problem we faced was loading the .csv file from the IC2S2 website using web scraping. Due to the fact that the website's dataset was transformed whenever we web scraped, we needed to locate the dataset and extract it before web scraping in Python. This wasn't the biggest issue as we could extract the HTML information from the website and locate it, but it proved to be a hassle to find it without downloading all dependencies (i.e.  the whole website)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d0a01",
   "metadata": {},
   "source": [
    "### Part 3: Collect Research Articles ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_df = df_auth[(df_auth[\"works_count\"] >= 5) & (df_auth[\"works_count\"] <= 5000)].copy()\n",
    "eligible_authors = set(eligible_df[\"id\"].dropna().tolist())\n",
    "\n",
    "print(\"Eligible authors:\", len(eligible_authors))\n",
    "display(eligible_df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for level-0 concept IDs\n",
    "\n",
    "social_names = [\"Sociology\", \"Psychology\", \"Economics\", \"Political science\"]\n",
    "quant_names  = [\"Mathematics\", \"Physics\", \"Computer science\"]\n",
    "\n",
    "social_ids = [get_concept_id_by_name(n) for n in social_names]\n",
    "quant_ids  = [get_concept_id_by_name(n) for n in quant_names]\n",
    "\n",
    "social_ids = [x for x in social_ids if x]\n",
    "quant_ids  = [x for x in quant_ids if x]\n",
    "\n",
    "assert len(social_ids) > 0, \"Could not find any social concept IDs.\"\n",
    "assert len(quant_ids)  > 0, \"Could not find any quantitative concept IDs.\"\n",
    "\n",
    "# OpenAlex OR within a single filter uses \"|\"\n",
    "social_or = \"|\".join(social_ids)\n",
    "quant_or  = \"|\".join(quant_ids)\n",
    "\n",
    "print(\"Social concept OR:\", social_or)\n",
    "print(\"Quant concept OR:\", quant_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_works_for_author_filtered(author_id, max_pages=None):\n",
    "    filter_str = (\n",
    "        f\"authorships.author.id:{author_id},\"\n",
    "        f\"cited_by_count:>10,\"\n",
    "        f\"concept.id:{social_or},\"\n",
    "        f\"concept.id:{quant_or}\"\n",
    "    )\n",
    "    params = {\n",
    "        \"filter\": filter_str,\n",
    "        \"select\": \"id,publication_year,cited_by_count,authorships,title,abstract_inverted_index\",\n",
    "        \"sort\": \"cited_by_count:desc\",\n",
    "    }\n",
    "    yield from paginate(\"/works\", params, per_page=200, max_pages=max_pages)\n",
    "\n",
    "def extract_author_ids(work):\n",
    "    auths = work.get(\"authorships\") or []\n",
    "    ids = []\n",
    "    for a in auths:\n",
    "        au = (a.get(\"author\") or {}).get(\"id\")\n",
    "        if au:\n",
    "            ids.append(au)\n",
    "    return ids\n",
    "\n",
    "D2_rows, D3_rows = [], []\n",
    "seen_work_ids = set()\n",
    "\n",
    "n_seen = 0\n",
    "n_kept = 0\n",
    "\n",
    "for aid in eligible_authors:\n",
    "    for w in iter_works_for_author_filtered(aid, max_pages=None):\n",
    "        n_seen += 1\n",
    "        wid = w.get(\"id\")\n",
    "        if not wid or wid in seen_work_ids:\n",
    "            continue\n",
    "\n",
    "        # More than 10 citations filter\n",
    "        c = w.get(\"cited_by_count\")\n",
    "        if c is None or c <= 10:\n",
    "            continue\n",
    "\n",
    "        # Fewer than 10 authors filter\n",
    "        author_count = len(w.get(\"authorships\") or [])\n",
    "        if author_count >= 10:\n",
    "          continue\n",
    "\n",
    "        author_ids = extract_author_ids(w)\n",
    "\n",
    "        # keep only if at least one eligible IC2S2 author is on the paper (usually true)\n",
    "        if not any(x in eligible_authors for x in author_ids):\n",
    "            continue\n",
    "\n",
    "        D2_rows.append({\n",
    "            \"id\": wid,\n",
    "            \"publication_year\": w.get(\"publication_year\"),\n",
    "            \"cited_by_count\": c,\n",
    "            \"author_ids\": author_ids\n",
    "        })\n",
    "\n",
    "        D3_rows.append({\n",
    "            \"id\": wid,\n",
    "            \"title\": w.get(\"title\"),\n",
    "            \"abstract_inverted_index\": w.get(\"abstract_inverted_index\")\n",
    "        })\n",
    "\n",
    "        seen_work_ids.add(wid)\n",
    "        n_kept += 1\n",
    "\n",
    "print(\"Works returned (including duplicates across authors):\", n_seen)\n",
    "print(\"Unique works kept:\", n_kept)\n",
    "\n",
    "df2 = pd.DataFrame(D2_rows)\n",
    "df3 = pd.DataFrame(D3_rows)\n",
    "\n",
    "print(\"D2 shape:\", df2.shape)\n",
    "print(\"D3 shape:\", df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fc71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df2.head(20))\n",
    "display(df3.head(20))\n",
    "\n",
    "print(\"Duplicate IDs in D2?\", df2[\"id\"].duplicated().any())\n",
    "print(\"Duplicate IDs in D3?\", df3[\"id\"].duplicated().any())\n",
    "print(\"Same ID sets?\", set(df2[\"id\"]) == set(df3[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset D2/D3 summary for report\n",
    "\n",
    "# Number of works in D2\n",
    "n_works = len(df2)\n",
    "\n",
    "# Number of unique authors across all works in D2\n",
    "all_author_ids = set()\n",
    "for ids in df2[\"author_ids\"]:\n",
    "    for aid in ids:\n",
    "        all_author_ids.add(aid)\n",
    "\n",
    "n_authors = len(all_author_ids)\n",
    "\n",
    "print(f\"Number of works in D2: {n_works}\")\n",
    "print(f\"Number of unique authors in D2: {n_authors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e1d5a",
   "metadata": {},
   "source": [
    "***How many works are listed in your Dataset D2 (IC2S2 papers) dataframe? How many unique researchers have co-authored these works?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a6feb",
   "metadata": {},
   "source": [
    "This is also answerd in the cell above. There are 1367 works in D2 and the number of unique authors that have co-authored are 2284"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc0f29",
   "metadata": {},
   "source": [
    "***Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23832005",
   "metadata": {},
   "source": [
    "We used a limit of \"max_authors\" to make sure we had enough API requests, as we have had to deal with that problem a lot. The filter for citations makes sure that OpenAlex only takes \"more relevant\" articles, which speeds up the process significantly. The \"author_count\" filter makes sure we only focus on \"smaller collaborations\" and don't focus on very large ones. The filter regarding \"eligible authors\" also filters out a lot, as it focuses on authors that have contributed a significant amount, which helps to signify their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab21834",
   "metadata": {},
   "source": [
    "***Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aac08a",
   "metadata": {},
   "source": [
    "Before applying any of the filters or specific settings, we ran a testing round on just one person, which alone gave us an enormous output, and it was very difficult to find specific information in it. This alone made us realize how necessary it would be to apply filters when searching for multiple people, but in practice, we also realised just how powerful of a tool it can be, especially when there is very specific information you are interested in or a specific criteria or threshold you want to limit yourself to. Overall, we think they are an excellent tool.  \n",
    "\n",
    "However, we also see some potential problem when applying these filters. As an example, if you choose to filter based on a work or an author that have been cited at least x amount of times, you may underrepresent authors or works that are legitimately good but have just not been used much or discovered yet. Moreover, this can overrepresent works or authors that may not be good anymore or have become outdated, just because they have been cited a lot. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
